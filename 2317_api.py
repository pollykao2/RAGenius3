from flask import Flask, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import os

app = Flask(__name__)
CORS(app)

os.environ.pop("OPENAI_API_KEY", None)
load_dotenv(dotenv_path="C:/Users/user/Desktop/RAGenius3/.env")

import time
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
from openai import OpenAI
import yfinance as yf
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
import string
from nltk.tokenize import TreebankWordTokenizer
import ta
from datetime import datetime, timedelta

# æŠ“å–æ–°èå…§å®¹ä¸¦éæ¿¾è¿‘30å¤©

def extract_news_content(url):
    try:
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        paragraphs = [p.get_text() for p in soup.find_all('p')]
        content = ' '.join(paragraphs).strip()

        # æŠ“ metadata ç™¼å¸ƒæ™‚é–“
        published_str = None
        time_tag = soup.find('time')
        if time_tag and time_tag.has_attr('datetime'):
            published_str = time_tag['datetime']
        elif soup.find('meta', attrs={"property": "article:published_time"}):
            published_str = soup.find('meta', attrs={"property": "article:published_time"})['content']
        elif soup.find('meta', attrs={"name": "pubdate"}):
            published_str = soup.find('meta', attrs={"name": "pubdate"})['content']

        if published_str:
            try:
                dt = datetime.fromisoformat(published_str.replace('Z', '+00:00')).astimezone()
                if dt < datetime.now() - timedelta(days=30):
                    return ""
            except:
                pass
        return content
    except Exception as e:
        print(f"âŒ ç„¡æ³•æŠ“å–æ–°è {url}: {e}")
        return ""

# Google Programmable Search API

def search_google_news(query, num_results=10):
    api_key = os.getenv("GOOGLE_API_KEY")
    cse_id = os.getenv("GOOGLE_CSE_ID")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cse_id,
        "q": query,
        "num": num_results
    }
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        return r.json().get('items', [])
    except Exception as e:
        print(f"âŒ æœå°‹å¤±æ•—ï¼š{e}")
        return []

@app.route("/api/stock")
def stock_data():
    start = time.perf_counter()
    tokenizer = TreebankWordTokenizer()

    # Pinecone & æ¨¡å‹åˆå§‹åŒ–
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index = pc.Index("news-index")
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')
    sentiment_pipe = pipeline(
        "sentiment-analysis",
        model="distilbert/distilbert-base-uncased-finetuned-sst-2-english"
    )

    # æŠ“è¿‘30å¤©æ–°è
    date_after = (datetime.utcnow() - timedelta(days=30)).strftime("%Y-%m-%d")
    query_str = f"é´»æµ· after:{date_after}"
    results = search_google_news(query_str, num_results=15)

    filtered = []
    for it in results:
        title = it.get('title', '')
        link = it.get('link', '')
        content = extract_news_content(link)
        if content:
            filtered.append({'title': title, 'link': link, 'content': content})
    filtered = filtered[:10]

    # BM25 å‰è™•ç†
    corpus = []
    for e in filtered:
        txt = e['title'] + ' ' + e['content'][:1000]
        toks = [t for t in tokenizer.tokenize(txt.lower()) if t not in string.punctuation]
        corpus.append(toks)
    bm25 = BM25Okapi(corpus)

    # ä¸Šå‚³å‘é‡åˆ° Pinecone
    for i, e in enumerate(filtered):
        full = e['title'] + ' ' + e['content'][:1000]
        vec = model.encode(full).tolist()
        s = sentiment_pipe(full)[0]
        index.upsert([{  
            'id': f'news-{i}',
            'values': vec,
            'metadata': {
                'text': full,
                'url': e['link'],
                'sentiment': s['label'],
                'sentiment_score': round(s['score'], 3)
            }
        }])

    # BM25 + Pinecone æŸ¥è©¢
    q = "é´»æµ·AIç™¼å±•å°å¸‚å ´å½±éŸ¿"
    qtokens = [t for t in tokenizer.tokenize(q.lower()) if t not in string.punctuation]
    bm_scores = bm25.get_scores(qtokens)
    top5 = sorted(range(len(bm_scores)), key=lambda i: bm_scores[i], reverse=True)[:5]

    qvec = model.encode(q).tolist()
    res = index.query(vector=qvec, top_k=3, include_metadata=True)
    matches = res.get('matches', [])

    pos = sum(1 for m in matches if m['metadata']['sentiment']=='POSITIVE')
    neg = sum(1 for m in matches if m['metadata']['sentiment']=='NEGATIVE')
    avg_sent = 'æ­£å‘' if pos>neg else 'è² å‘' if neg>pos else 'ä¸­æ€§'

        # === ä¿ç•™åŸ prompt ä¸¦æ­£ç¢ºåŒ…ä¸‰é‡å¼•è™Ÿã€ç”¨å–®å¤§æ‹¬è™Ÿæ’è®Šæ•¸ ===
    prompt_content = f"""
ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå¾ä»¥ä¸‹äº”é¢å‘ï¼šè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ï¼Œ
**ä½ çš„æ–°èå¿…é ˆæ˜¯è¿‘30æ—¥çš„**ï¼Œ
çµåˆè‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ã€å·´è²ç‰¹è­·åŸæ²³ã€å½¼å¾—æ—å€é¸è‚¡æ–¹æ³•ï¼Œä»¥åŠç¸½é«”ç¶“æ¿Ÿåˆ©ç‡æŠ˜ç¾ç†è«–ï¼Œ
é‡å° {stock_name} çš„è©³ç´°æŠ•è³‡æ•¸æ“šèˆ‡æŠ€è¡“æŒ‡æ¨™é€²è¡Œåˆ†æï¼Œä¸¦åš´æ ¼ä¾æ“šæ•¸æ“šåšå°ˆæ¥­æ¨è«–ã€‚
**é‚è¼¯æ¨è«–** èˆ‡ **å…·é«”é æ¸¬**

ä»¥ä¸‹æ˜¯è¨ˆç®—çµæœï¼š
- EPS: {eps:.2f}ï¼Œæœ¬ç›Šæ¯”(PE): {pe_ratio:.2f}ï¼Œç”¢æ¥­å¹³å‡PE: {industry_pe:.2f}
- ä½¿ç”¨ DCF æ¨¡å‹ (FCF={fcf}å„„, g={growth_rate}%, r={discount_rate}%) ä¼°å€¼ç´„ç‚º {intrinsic_value:.2f}
- ç¾åƒ¹: {current_price:.2f}ï¼Œä¼°å€¼å·®è·ç´„ {margin:.1f}%
- ROE: éå»5å¹´å¹³å‡ {roe:.2f}% â†’ è­·åŸæ²³: {moat}
- RSI: {rsi:.2f} â†’ {rsi_signal}ï¼›MACD: {macd:.2f}/{macd_signal:.2f} â†’ {macd_signal_text}
- ç•¶å‰åˆ©ç‡: {interest_rate:.2f}% ï¼Œè™•æ–¼ {rate_position}ã€‚
- æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: æ­£å‘/è² å‘/ä¸­æ€§

ğŸ“°ã€æ–°èæƒ…ç·’ã€‘
- æœ¬æ—¥æ–°èæƒ…ç·’å¹³å‡ï¼š{avg_sent}
- é‡é»æ–°èæ‘˜è¦å¦‚ä¸‹ï¼š
"""  

    # æŠŠæ¯ç¯‡ match çš„ text æ¥åœ¨ prompt å¾Œé¢
    for m in matches:
        prompt_content += m["metadata"]["text"] + "\n"

    # ç¬¬äºŒæ®µä¹Ÿç”¨ä¸‰é‡å¼•è™ŸåŒ…èµ·ä¾†
    prompt_content += """
ä½ éœ€è¦å°‡ä¸Šè¿°æ•¸æ“šèˆ‡æ–°èå®Œæ•´èåˆï¼Œå¾ã€Œè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ã€
äº”å¤§é¢å‘é€²è¡Œå°ˆæ¥­åˆ¤è®€ï¼Œä¸¦çµåˆï¼š
- è‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ï¼ˆä½ä¼°é«˜å®‰å…¨é‚Šéš›ï¼‰
- å·´è²ç‰¹è­·åŸæ²³ï¼ˆROEç©©å¥ã€ç¨ä½”å„ªå‹¢ï¼‰
- å½¼å¾—æ—å€é¸è‚¡ï¼ˆç”Ÿæ´»åŒ–æ´å¯Ÿèˆ‡æˆé•·æ€§ï¼‰
- åˆ©ç‡æŠ˜ç¾èˆ‡é€šè†¨é æœŸï¼ˆå®è§€ç¶“æ¿Ÿè§’åº¦ï¼‰

å¦å¤–è£œå……ä¸€äº›å¸‚å ´å¸¸è­˜ï¼Œä½ å¯ä»¥åœ¨æ¨è«–æ™‚ä¸€ä½µè€ƒæ…®ï¼š
- è‹¥è¯æº–æœƒ(Fed)é™æ¯ï¼Œé€šå¸¸è¢«è¦–ç‚ºå°è‚¡ç¥¨å¸‚å ´åå¤šçš„åˆ©å¤šã€‚
- è‹¥CPIæŒçºŒä¸Šæ¼²ï¼Œé¡¯ç¤ºé€šè†¨å£“åŠ›å¢åŠ ï¼Œé€šå¸¸å°è‚¡å¸‚åç©ºã€‚
- è‹¥ä¼æ¥­æŒçºŒæŠ•å…¥AIèˆ‡è‡ªå‹•åŒ–ï¼Œä»£è¡¨æœªä¾†æˆé•·æ½›åŠ›ï¼Œååˆ©å¤šã€‚
- æŠ€è¡“é¢è‹¥RSIéä½ã€MACDé»ƒé‡‘äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½åå½ˆï¼›è‹¥RSIéé«˜æˆ–MACDæ­»äº¡äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½å›æª”ã€‚
- æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: {avg_sent}

ä½ éœ€è¦æ˜ç¢ºçš„æŒ‡å‡º
1. æ˜ç¢ºæ–°èä¾†æºï¼šåœ¨æç¤ºä¸­æ¨™æ˜æ–°èå¹³å°ï¼æ¨™é¡Œï¼æ™‚é–“ã€‚
2. èšç„¦é‡é»å…ƒç´ ï¼šå…ˆã€Œæ‘˜è¦æ–°èè¦é»ã€ï¼Œå†åŸºæ–¼æ­¤æ¨æ¸¬å¿ƒç†ã€‚
3. æ¡†æ¶åŒ–åˆ†æç¶­åº¦ï¼šå¾ã€Œæƒ…ç·’(Fear/Greed)ã€ã€Œé æœŸ(ä¸Šæ¼²/ä¸‹è·Œ)ã€ã€Œå‹•æ©Ÿ(é€¢ä½å¸ƒå±€/ç²åˆ©äº†çµ)ã€ç­‰æ‹†è§£ã€‚
4. ç”¢å‡ºæ ¼å¼å¼•å°ï¼šç”¨æ¢åˆ—æˆ–åˆ†æ®µå›è¦†ã€‚

æ­¤å¤–ï¼Œ**è«‹å‹™å¿…é æ¸¬æ˜æ—¥çš„è‚¡åƒ¹æ–¹å‘èˆ‡é ä¼°å¹…åº¦**ï¼ˆä¾‹å¦‚ï¼šã€Œé ä¼°æ˜æ—¥å°å¹…ä¸Šæ¼²ç´„1%ã€æˆ–ã€Œé æœŸä¸‹è·Œ1~2%ã€ï¼‰ï¼Œ
æœ€å¾Œä»¥1â€“2å¥è©±çµ¦å‡ºç°¡çŸ­çš„æŠ•è³‡å»ºè­°ï¼ˆå¦‚ã€Œå¯é€¢ä½åˆ†æ‰¹ä½ˆå±€ã€æˆ–ã€Œå»ºè­°çŸ­æœŸè§€æœ›ã€ï¼‰ï¼Œ
ç¦æ­¢åªæ³›æ³›è€Œè«‡ï¼Œå¿…é ˆå…·é«”è¼¸å‡ºæ¨è«–èˆ‡é æ¸¬æ•¸æ“šã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
"""


    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role":"user","content": prompt_content}]
    )

    return jsonify({
        'news': [{'title': e['title'], 'link': e['link']} for e in filtered],
        'sentiment_summary': avg_sent,
        'gpt': response.choices[0].message.content
    })

if __name__ == '__main__':
    app.run(port=5000)