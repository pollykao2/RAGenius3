from flask import Flask, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import os
import time
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
from openai import OpenAI
import yfinance as yf
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
import string
from nltk.tokenize import TreebankWordTokenizer
import ta
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from newsapi import NewsApiClient
from ta.momentum import RSIIndicator
from ta.trend import MACD
from urllib.parse import urlencode


# åˆå§‹åŒ– Flask æ‡‰ç”¨èˆ‡ç’°å¢ƒè®Šæ•¸
app = Flask(__name__)
CORS(app)
os.environ.pop("OPENAI_API_KEY", None)
load_dotenv(dotenv_path="C:/Users/user/Desktop/RAGenius3/.env")


def extract_news_content(url: str) -> str:
    """
    é€é requests + BeautifulSoup æŠ“å–ç¶²é å…§æ‰€æœ‰ <p> æ®µè½æ–‡å­—ï¼Œ
    ä¸¦æ ¹æ“š meta time æ¨™ç±¤æˆ– <time> åˆ¤æ–·æ˜¯å¦è¿‘30å¤©ï¼Œè¶…éè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    try:
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        # æ”¶é›†æ®µè½æ–‡å­—
        paragraphs = [p.get_text() for p in soup.find_all('p')]
        content = ' '.join(paragraphs).strip()

        # æŠ“ç™¼å¸ƒæ™‚é–“
        published_str = None
        time_tag = soup.find('time')
        if time_tag and time_tag.has_attr('datetime'):
            published_str = time_tag['datetime']
        elif soup.find('meta', attrs={"property": "article:published_time"}):
            published_str = soup.find('meta', attrs={"property": "article:published_time"})['content']
        elif soup.find('meta', attrs={"name": "pubdate"}):
            published_str = soup.find('meta', attrs={"name": "pubdate"})['content']

        if published_str:
            try:
                dt = datetime.fromisoformat(published_str.replace('Z', '+00:00')).astimezone()
                if dt < datetime.now() - timedelta(days=10):
                    return "", None
                return content, dt
            except:
                pass
        return content, None
    except Exception as e:
        print(f"âŒ ç„¡æ³•æŠ“å–æ–°è {url}: {e}")
        return ""


def search_google_news(query: str, num_results: int = 20):
    """
    ä½¿ç”¨ Google Programmable Search API é€²è¡Œé—œéµå­—æœå°‹ï¼Œ
    ä¸å« dateRestrict èˆ‡ after:ï¼Œç”± extract_news_content è™•ç†æ—¥æœŸéæ¿¾ã€‚
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    cse_id  = os.getenv("GOOGLE_CSE_ID")

    url= "https://www.googleapis.com/customsearch/v1"

    per_page = 10
    total_pages = (num_results + per_page - 1) // per_page  # å‘ä¸Šå–æ•´æ•¸

    params_list = []
    for i in range(total_pages):
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
            "num": per_page,
            "start": i * per_page + 1,
            "hl": "zh-TW",
            "gl": "tw"
        }
        params_list.append(params)

        headers = {
            "Accept": "application/json",
            "User-Agent": "Mozilla/5.0"
        }
        
        all_items = []

    for i, params in enumerate(params_list):
        encoded = urlencode(params, safe="")
        full_url = f"{url}?{encoded}"
        print(f"ğŸ”— æœ€çµ‚ URL [{i+1}]:", full_url)

        response = requests.get(full_url, headers=headers, timeout=10)

        print("ğŸ” Google å›å‚³ç‹€æ…‹ç¢¼ï¼š", response.status_code)
        print("ğŸ§¾ Google å›å‚³å…§å®¹ï¼ˆå‰500å­—ï¼‰:", response.text[:500])

        try:
            response.raise_for_status()
        except Exception as e:
            print(f"âš ï¸ ç‹€æ…‹éŒ¯èª¤ï¼š{e}")
            continue

        results = response.json()
        if "error" in results:
            print("âŒ Google API å›å‚³éŒ¯èª¤ï¼š", results["error"].get("message"))
            continue

        items = results.get("items", [])
        all_items.extend(items)

    if not all_items:
        print("âš ï¸ æ‰€æœ‰åˆ†é éƒ½æ²’æŠ“åˆ° items")
        return []

    print(f"âœ… æˆåŠŸå–å¾— {len(all_items)} ç­†æœå°‹çµæœ")



    return [
        {
            "title": item.get("title"),
            "link": item.get("link"),
            "snippet": item.get("snippet", "")
        }
        for item in all_items
    ]

def get_stock_metrics(stock_symbol: str, newsapi_key: str) -> dict:
    """
    å‹•æ…‹å¾ Yahoo Finance æŠ“å– EPSã€PEã€DCFã€ROEã€RSIã€MACDã€åˆ©ç‡ç­‰ï¼Œ
    ä¸¦å¾ NewsAPI æŠ“å–è¿‘ 30 å¤©æ–°èåšæƒ…ç·’åˆ†æã€‚
    """
    ticker = yf.Ticker(stock_symbol)
    info   = ticker.info

    # EPSã€PE
    eps      = info.get("trailingEps", np.nan)
    pe_ratio = info.get("trailingPE", np.nan)

    # ç”¢æ¥­å¹³å‡ PE
    peers = info.get("industryPeers", [])
    peers_pe = []
    for peer in peers:
        try:
            p = yf.Ticker(peer).info.get("trailingPE")
            if p and p>0:
                peers_pe.append(p)
        except:
            continue
    industry_pe = float(np.mean(peers_pe)) if peers_pe else np.nan

    # DCF ä¼°å€¼ï¼ˆFree Cash Flowï¼‰
    row_label = "Free Cash Flow"
    try:
        if row_label in ticker.cashflow.index:
            cf_series = ticker.cashflow.loc[row_label]
        else:
            cf_series = pd.Series(dtype=float)

        cf = cf_series / 1e8
        fcf = cf.iloc[0] if len(cf) else np.nan
        growth_rate = ((cf.iloc[0]/cf.iloc[1] - 1)*100) if len(cf) >= 2 and cf.iloc[1] != 0 else np.nan
    except Exception as e:
        print(f"âŒ å–å¾— FCF/Growth Rate éŒ¯èª¤ï¼š{e}")
        fcf = np.nan
        growth_rate = np.nan

    # æŠ˜ç¾ç‡
    try:
        tnx = yf.Ticker("^TNX").history(period="1d")["Close"].iloc[-1] / 10
    except:
        tnx = 3.5  # fallback å‡è¨­åˆ©ç‡

    discount_rate = tnx / 100 + 0.05
    try:
        intrinsic_value = fcf * (1 + growth_rate/100) / (discount_rate - growth_rate/100)
    except:
        intrinsic_value = np.nan


    # å³æ™‚åƒ¹æ ¼ & ä¼°å€¼å·®è·
    current_price = ticker.history(period="1d")["Close"].iloc[-1]
    margin = (current_price - intrinsic_value) / intrinsic_value * 100

    # ROE & è­·åŸæ²³
    if "Net Income" in ticker.financials.index:
        netinc = ticker.financials.loc["Net Income"]
    else:
        netinc = pd.Series(dtype=float)

    if "Total Stockholder Equity" in ticker.balance_sheet.index:
        equity = ticker.balance_sheet.loc["Total Stockholder Equity"]
    else:
        equity = pd.Series(dtype=float)

    roe_series = (netinc / equity * 100).dropna()
    roe = float(roe_series.head(5).mean()) if len(roe_series) else np.nan
    moat = "å¼·" if roe>15 else "ä¸­" if roe>5 else "å¼±"

    # RSI & MACD
    hist60 = ticker.history(period="60d")["Close"]
    rsi = float(RSIIndicator(hist60, window=14).rsi().iloc[-1])
    rsi_signal = "è¶…è²·" if rsi>70 else "è¶…è³£" if rsi<30 else "ä¸­æ€§"
    macd_ind = MACD(hist60)
    macd = float(macd_ind.macd().iloc[-1])
    macd_signal = float(macd_ind.macd_signal().iloc[-1])
    macd_signal_text = "è²·å…¥" if macd>macd_signal else "è³£å‡º"

    # åˆ©ç‡ä½ç½®
    interest_rate = tnx
    rate_position = "åé«˜" if interest_rate>3 else "åä½" if interest_rate<1 else "æŒå¹³"

    # æ–°èæƒ…ç·’ï¼ˆNewsAPIï¼‰
    newsapi = NewsApiClient(api_key=newsapi_key)
    to_date = datetime.utcnow().date()
    from_date = to_date - timedelta(days=30)
    articles = newsapi.get_everything(
        q=stock_symbol,
        from_param=from_date.isoformat(),
        to=to_date.isoformat(),
        language="zh",
        sort_by="relevancy",
        page_size=15
    ).get("articles", [])
    headlines = [a.get("title","") for a in articles]
    sa_pipe = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    sentiments = sa_pipe(headlines)
    scores = [1 if s["label"]=="POSITIVE" else -1 for s in sentiments]
    avg_score = sum(scores)/len(scores) if scores else 0
    avg_sent = "æ­£å‘" if avg_score>0 else "è² å‘" if avg_score<0 else "ä¸­æ€§"

    return {
        "eps": eps,
        "pe_ratio": pe_ratio,
        "industry_pe": industry_pe,
        "fcf": fcf,
        "growth_rate": growth_rate,
        "discount_rate": discount_rate*100,
        "intrinsic_value": intrinsic_value,
        "current_price": current_price,
        "margin": margin,
        "roe": roe,
        "moat": moat,
        "rsi": rsi,
        "rsi_signal": rsi_signal,
        "macd": macd,
        "macd_signal": macd_signal,
        "macd_signal_text": macd_signal_text,
        "interest_rate": interest_rate,
        "rate_position": rate_position,
        "avg_sent": avg_sent,
        "headlines": headlines
    }

@app.route("/api/stock")
def stock_data():
    tokenizer = TreebankWordTokenizer()

    # Pinecone èˆ‡ Embedding & Sentiment Pipeline
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index = pc.Index("news-index")
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')
    sentiment_pipe = pipeline(
        "sentiment-analysis",
        model="distilbert/distilbert-base-uncased-finetuned-sst-2-english",
        revision="714eb0f"

    )

    # 1) æœæ–°è
    print("âœ… GOOGLE_API_KEY:", os.getenv("GOOGLE_API_KEY"))
    print("âœ… GOOGLE_CSE_ID:", os.getenv("GOOGLE_CSE_ID"))
    results = search_google_news("é´»æµ·", num_results=10)
    filtered = []
    for it in results:
        title = it.get('title', '')
        link = it.get('link', '')
        print(f"ğŸ”— å˜—è©¦æŠ“æ–°èï¼š{title} | {link}")

        content, pub_dt = extract_news_content(link)
        if content:
            sentiment = sentiment_pipe(title)[0]
            filtered.append({
                'title': title,
                'link': link,
                'content': content,
                'sentiment': sentiment['label'],
                'score': round(sentiment['score'], 3),
                'published': pub_dt
            })

    filtered.sort(key=lambda x: x['published'] or datetime.min, reverse=True)
    filtered = filtered[:10]

    # 2) BM25 å‰è™•ç†
    corpus = []
    for e in filtered:
        txt = e['title'] + ' ' + e['content'][:1000]
        toks = [t for t in tokenizer.tokenize(txt.lower()) if t not in string.punctuation]
        corpus.append(toks)

    # 3) ç©º corpus guard
    if not corpus or all(len(doc)==0 for doc in corpus):
        matches = []
        avg_sent = "ä¸­æ€§"
    else:
        # 4) BM25
        bm25 = BM25Okapi(corpus)
        # 5) ä¸Šå‚³å‘é‡
        for i,e in enumerate(filtered):
            text = e['title'] + ' ' + e['content'][:1000]
            vec  = model.encode(text).tolist()
            short_text = text[:500]  # æˆ–ä¿å®ˆé»ç”¨ 300
            s = sentiment_pipe(short_text)[0]
            
            index.upsert([{
                'id': f'news-{i}',
                'values': vec,
                'metadata': {
                'text': text,
                'url': e['link'],
                'sentiment': s['label'],
                'sentiment_score': round(s['score'],3)
                }
            }])
        # 6) BM25 å– top5ï¼Œä¸¦åšå‘é‡æª¢ç´¢ top3
        query   = "é´»æµ·AIç™¼å±•å°å¸‚å ´å½±éŸ¿"
        qtokens = [t for t in tokenizer.tokenize(query.lower())
                    if t not in string.punctuation]
        bm_scores = bm25.get_scores(qtokens)
        top_idx   = sorted(range(len(bm_scores)),
                        key=lambda i: bm_scores[i],
                        reverse=True)[:5]

        qvec    = model.encode(query).tolist()
        res     = index.query(vector=qvec, top_k=3, include_metadata=True)
        matches = res.get('matches', [])

        # 7) å¹³å‡æƒ…ç·’
        pos = sum(1 for m in matches
                if m['metadata']['sentiment'] == 'POSITIVE')
        neg = sum(1 for m in matches
                if m['metadata']['sentiment'] == 'NEGATIVE')
        avg_sent = 'æ­£å‘' if pos > neg else 'è² å‘' if neg > pos else 'ä¸­æ€§'

    # è²¡å‹™èˆ‡æŠ€è¡“æŒ‡æ¨™
    stock_symbol = "2317.TW"
    stock_name   = "é´»æµ·"
    metrics = get_stock_metrics(stock_symbol, os.getenv("NEWSAPI_KEY"))

    eps               = metrics['eps']; pe_ratio          = metrics['pe_ratio']; industry_pe       = metrics['industry_pe']
    fcf               = metrics['fcf']; growth_rate       = metrics['growth_rate']; discount_rate     = metrics['discount_rate']
    intrinsic_value   = metrics['intrinsic_value']; current_price     = metrics['current_price']
    margin            = metrics['margin']; roe               = metrics['roe']; moat              = metrics['moat']
    rsi               = metrics['rsi']; rsi_signal        = metrics['rsi_signal']
    macd              = metrics['macd']; macd_signal       = metrics['macd_signal']; macd_signal_text  = metrics['macd_signal_text']
    interest_rate     = metrics['interest_rate']; rate_position     = metrics['rate_position']
    avg_sent_fin      = metrics['avg_sent']; headlines         = metrics['headlines']

    # çµ„ prompt ä¸¦å‘¼å« GPT
    prompt_content = f"""
        ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå¾ä»¥ä¸‹äº”é¢å‘ï¼šè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ï¼Œ
        **ä½ çš„æ–°èå¿…é ˆæ˜¯è¿‘30æ—¥çš„**ï¼Œ
        çµåˆè‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ã€å·´è²ç‰¹è­·åŸæ²³ã€å½¼å¾—æ—å€é¸è‚¡æ–¹æ³•ï¼Œä»¥åŠç¸½é«”ç¶“æ¿Ÿåˆ©ç‡æŠ˜ç¾ç†è«–ï¼Œ


        é‡å° {stock_name} çš„è©³ç´°æŠ•è³‡æ•¸æ“šèˆ‡æŠ€è¡“æŒ‡æ¨™é€²è¡Œåˆ†æï¼Œä¸¦åš´æ ¼ä¾æ“šæ•¸æ“šåšå°ˆæ¥­æ¨è«–ã€‚
        **é‚è¼¯æ¨è«–** èˆ‡ **å…·é«”é æ¸¬**

        ä»¥ä¸‹æ˜¯è¨ˆç®—çµæœï¼š
        - EPS: {eps:.2f}ï¼Œæœ¬ç›Šæ¯”(PE): {pe_ratio:.2f}ï¼Œç”¢æ¥­å¹³å‡PE: {industry_pe:.2f}
        - ä½¿ç”¨ DCF æ¨¡å‹ (FCF={fcf:.2f}å„„, g={growth_rate:.1f}%, r={discount_rate:.2f}%) ä¼°å€¼ç´„ç‚º {intrinsic_value:.2f}
        - ç¾åƒ¹: {current_price:.2f}ï¼Œä¼°å€¼å·®è·ç´„ {margin:.1f}%
        - ROE: éå»5å¹´å¹³å‡ {roe:.2f}% â†’ è­·åŸæ²³: {moat}
        - RSI: {rsi:.2f} â†’ {rsi_signal}ï¼›MACD: {macd:.2f}/{macd_signal:.2f} â†’ {macd_signal_text}
        - ç•¶å‰åˆ©ç‡: {interest_rate:.2f}%ï¼Œè™•æ–¼ {rate_position}
        - æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: {avg_sent}

        ğŸ“°ã€æ–°èæƒ…ç·’ã€‘
        - æœ¬æ—¥æ–°èæƒ…ç·’å¹³å‡ï¼š{avg_sent}
        - é‡é»æ–°èæ‘˜è¦å¦‚ä¸‹ï¼š
        1. {headlines[0]}
        2. {headlines[1]}
        3. â€¦
        """
    for m in matches:
        prompt_content += m['metadata']['text'] + "\n"

    prompt_content += """
        ä½ éœ€è¦å°‡ä¸Šè¿°æ•¸æ“šèˆ‡æ–°èå®Œæ•´èåˆï¼Œå¾ã€Œè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ã€
        äº”å¤§é¢å‘é€²è¡Œå°ˆæ¥­åˆ¤è®€ï¼Œä¸¦çµåˆï¼š
        - è‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ï¼ˆä½ä¼°é«˜å®‰å…¨é‚Šéš›ï¼‰
        - å·´è²ç‰¹è­·åŸæ²³ï¼ˆROEç©©å¥ã€ç¨ä½”å„ªå‹¢ï¼‰
        - å½¼å¾—æ—å€é¸è‚¡ï¼ˆç”Ÿæ´»åŒ–æ´å¯Ÿèˆ‡æˆé•·æ€§ï¼‰
        - åˆ©ç‡æŠ˜ç¾èˆ‡é€šè†¨é æœŸï¼ˆå®è§€ç¶“æ¿Ÿè§’åº¦ï¼‰

        å¦å¤–è£œå……ä¸€äº›å¸‚å ´å¸¸è­˜ï¼Œä½ å¯ä»¥åœ¨æ¨è«–æ™‚ä¸€ä½µè€ƒæ…®ï¼š
        - è‹¥è¯æº–æœƒ(Fed)é™æ¯ï¼Œé€šå¸¸è¢«è¦–ç‚ºå°è‚¡ç¥¨å¸‚å ´åå¤šçš„åˆ©å¤šã€‚
        - è‹¥CPIæŒçºŒä¸Šæ¼²ï¼Œé¡¯ç¤ºé€šè†¨å£“åŠ›å¢åŠ ï¼Œé€šå¸¸å°è‚¡å¸‚åç©ºã€‚
        - è‹¥ä¼æ¥­æŒçºŒæŠ•å…¥AIèˆ‡è‡ªå‹•åŒ–ï¼Œä»£è¡¨æœªä¾†æˆé•·æ½›åŠ›ï¼Œååˆ©å¤šã€‚
        - æŠ€è¡“é¢è‹¥RSIéä½ã€MACDé»ƒé‡‘äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½åå½ˆï¼›è‹¥RSIéé«˜æˆ–MACDæ­»äº¡äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½å›æª”ã€‚
        - æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: {avg_sent}
        

        ä½ éœ€è¦æ˜ç¢ºæŒ‡å‡ºï¼š
        1. æ˜ç¢ºæ–°èä¾†æºï¼ˆå¹³å°ï¼æ¨™é¡Œï¼æ™‚é–“ï¼‰ã€‚
        2. èšç„¦é‡é»å…ƒç´ ï¼ˆæ‘˜è¦æ–°èè¦é»ï¼Œå†æ¨æ¸¬å¿ƒç†ï¼‰ã€‚
        3. æ¡†æ¶åŒ–åˆ†æç¶­åº¦ï¼ˆæƒ…ç·’ã€é æœŸã€å‹•æ©Ÿï¼‰ã€‚
        4. ç”¢å‡ºæ ¼å¼å¼•å°ï¼ˆæ¢åˆ—æˆ–åˆ†æ®µï¼‰ã€‚
        5. **æ‰€æœ‰æ–°èçš†ä¾†è‡ªæœ€è¿‘10æ—¥ï¼Œä¸¦ä»¥è¿‘3æ—¥ç‚ºä¸»ã€‚**


        æ­¤å¤–ï¼Œ**è«‹å‹™å¿…é æ¸¬æ˜æ—¥çš„è‚¡åƒ¹æ–¹å‘èˆ‡é ä¼°å¹…åº¦**ï¼Œä¸¦ä»¥1â€“2å¥è©±çµ¦å‡ºæŠ•è³‡å»ºè­°ã€‚

        è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
        """

    # GPT å‘¼å«
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role":"user","content": prompt_content}],
    )

    ticker = yf.Ticker("2317.TW")

    chart_df = ticker.history(period="10d")[["Close"]].reset_index()
    chart_df["Date"] = chart_df["Date"].dt.strftime("%Y-%m-%d")  # æ™‚é–“æ ¼å¼è½‰ç‚ºå­—ä¸²
    chart_data = chart_df.to_dict(orient="records")

    return jsonify({
        'news': [{'title': e['title'], 'link': e['link'], 'sentiment': e['sentiment'], 'score': e['score']} for e in filtered],
        'sentiment_summary': avg_sent,
        'gpt': response.choices[0].message.content,
        'chart_data': chart_data 
    })

if __name__ == '__main__':
    app.run(port=5000, debug=True)
