from flask import Flask, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import os

app = Flask(__name__)
CORS(app)

os.environ.pop("OPENAI_API_KEY", None)

# å¼·åˆ¶è®€å–æ­£ç¢ºçš„ .env æª”æ¡ˆ
load_dotenv(dotenv_path="C:/Users/user/Desktop/RAGenius3/.env")


import time 
import torch
import matplotlib.pyplot as plt

import feedparser
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
from openai import OpenAI
import yfinance as yf
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
import string
from nltk.tokenize import TreebankWordTokenizer
import ta  

from datetime import datetime, timedelta
import time




    # ------------------------
    #  extract_news_content
def extract_news_content(url):
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = [p.get_text() for p in soup.find_all('p')]
        content = ' '.join(paragraphs)
        return content.strip()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return ""

@app.route("/api/stock")
def stock_data():
    start_time = time.perf_counter()
    # ------------------------
    # åˆå§‹åŒ– tokenizer
    tokenizer = TreebankWordTokenizer()


    # ------------------------
    # åˆå§‹åŒ– Pinecone
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index = pc.Index("news-index")

    # ------------------------
    # åˆå§‹åŒ– BERT å‘é‡åŒ–æ¨¡å‹
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')

    # ------------------------
    # Hugging Face æƒ…ç·’åˆ†æ
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model="distilbert/distilbert-base-uncased-finetuned-sst-2-english"
    )

    # ------------------------
    # Google News RSS (æœå°‹é´»æµ·)
    start_time = time.perf_counter() 
    # Google News RSS (æœå°‹é´»æµ·)
    rss_url = "https://news.google.com/rss/search?q=é´»æµ·"
    feed = feedparser.parse(rss_url)

    # ç¯©é¸è¿‘30æ—¥çš„æ–°è
    cutoff_date = datetime.now() - timedelta(days=30)
    filtered_entries = []
    news_list = []

    for entry in feed.entries:
        if hasattr(entry, "published_parsed"):
            published = datetime.fromtimestamp(time.mktime(entry.published_parsed))
            if published >= cutoff_date:
                filtered_entries.append(entry)
                news_list.append("[Google] " + entry.title + " - " + entry.link)

# æœ€å¤šåªå–10å‰‡
filtered_entries = filtered_entries[:10]

    # ------------------------
    # BM25 å‰ç½®è™•ç†
    tokenized_corpus = []
    for i, entry in enumerate(feed.entries[:10]):
        content = extract_news_content(entry.link)
        full_text = entry.title + " " + content[:1000]
        tokens = tokenizer.tokenize(full_text.lower())
    for i, entry in enumerate(feed.entries[:10]):
        content = extract_news_content(entry.link)
        full_text = entry.title + " " + content[:1000]
        tokens = tokenizer.tokenize(full_text.lower())
        tokens = [t for t in tokens if t not in string.punctuation]
        tokenized_corpus.append(tokens)

    bm25 = BM25Okapi(tokenized_corpus)
    print(f"[è¨ˆæ™‚] Google News & BM25 æº–å‚™è€—æ™‚: {time.perf_counter() - start_time:.2f} ç§’") 


    # ------------------------
    # æŠŠæ–°èè½‰å‘é‡ä¸¦ upsert åˆ° Pineconeï¼Œå¤šæ–°èå…§æ–‡çˆ¬å–
    start_time = time.perf_counter()
    for i, entry in enumerate(feed.entries[:10]):
        content = extract_news_content(entry.link)
        if not content:
            content = ""
            full_text = entry.title + " " + content[:1000]
            vector = model.encode(full_text).tolist()

            # åŸ·è¡Œæƒ…ç·’åˆ†æï¼ˆç”¨ full_text è€Œä¸æ˜¯ titleï¼‰
            sentiment_result = sentiment_pipeline(full_text)[0]
            sentiment_label = sentiment_result['label']      # POSITIVE or NEGATIVE
            sentiment_score = round(sentiment_result['score'], 3)

            # ä¸Šå‚³åˆ° Pineconeï¼ŒåŠ å…¥æƒ…ç·’æ¨™ç±¤
            index.upsert([{
                "id": f"news-{i}",
                "values": vector,
                "metadata": {
                    "text": full_text,
                    "url": entry.link,
                    "sentiment": sentiment_label,
                    "sentiment_score": sentiment_score
                }
            }])

    print(f"[è¨ˆæ™‚] å‘é‡ encode + Pinecone ä¸Šå‚³è€—æ™‚: {time.perf_counter() - start_time:.2f} ç§’") 
        # print(f" å·²ä¸Šå‚³åˆ° Pinecone: {text}")

    # ------------------------
    # ä½¿ç”¨ BM25 éæ¿¾å¾Œå†ç”¨å‘é‡æŸ¥è©¢
    start_time = time.perf_counter()
    query = "é´»æµ·AIç™¼å±•å°å¸‚å ´å½±éŸ¿"
    query_tokens = tokenizer.tokenize(query.lower())
    query_tokens = [t for t in query_tokens if t not in string.punctuation]

    bm25_scores = bm25.get_scores(query_tokens)
    sorted_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)

    print("\n=== BM25 æ’åºçµæœ (å‰5å) ===")
    # for idx in sorted_indices[:5]:
    #     print(f"Score: {bm25_scores[idx]:.4f}, Text: {news_list[idx]}")

    filtered_news = [news_list[i] for i in sorted_indices[:5]]

    # ------------------------
    # Pinecone èªæ„æŸ¥è©¢
    query_vector = model.encode(query).tolist()
    result = index.query(vector=query_vector, top_k=3, include_metadata=True)

    matches = result.get("matches", [])
    print(f"[è¨ˆæ™‚] Pinecone æª¢ç´¢è€—æ™‚: {time.perf_counter() - start_time:.2f} ç§’")  

    # === æ–°å¢é€™æ®µï¼šçµ±è¨ˆæƒ…ç·’å‚¾å‘ ===
    positive_count = sum(1 for m in matches if m['metadata'].get('sentiment') == 'POSITIVE')
    negative_count = sum(1 for m in matches if m['metadata'].get('sentiment') == 'NEGATIVE')

    if positive_count > negative_count:
        avg_sentiment_summary = "æ­£å‘"
    elif negative_count > positive_count:
        avg_sentiment_summary = "è² å‘"
    else:
        avg_sentiment_summary = "ä¸­æ€§"



    prompt_content = "ä»¥ä¸‹æ˜¯èˆ‡å•é¡Œæœ€ç›¸é—œçš„æ–°èï¼Œè«‹åˆ†æå®ƒå€‘å°é´»æµ·è‚¡åƒ¹çš„å½±éŸ¿ï¼š\n\n"
    for match in matches:
        sentiment = match["metadata"].get("sentiment", "æœªçŸ¥")
        score = match["metadata"].get("sentiment_score", 0.0)
        prompt_content += f"ã€æƒ…ç·’ï¼š{sentiment}ï¼Œä¿¡å¿ƒï¼š{score:.2f}ã€‘\n"
        prompt_content += match["metadata"]["text"] + "\n\n"

    # ------------------------
    # Yahoo Finance & æŠ€è¡“æŒ‡æ¨™ (ç”¨ ta)
    stock_symbol = "2317.TW"
    stock_name = "é´»æµ·"
    industry_pe = 14
    growth_rate = 5
    discount_rate = 8
    interest_rate = 1.875
    rate_position = "ä½åˆ©ç’°å¢ƒ" if interest_rate < 2 else "æ­£å¸¸æˆ–åé«˜"

    ticker = yf.Ticker(stock_symbol)
    info = ticker.info
    eps = info.get("trailingEps", 0)
    pe_ratio = info.get("trailingPE", 0)
    roe = 18
    moat = "ç©©å®š" if roe > 15 else "å¼±"
    fcf = 2800
    intrinsic_value = fcf * (1 + growth_rate/100) / ((discount_rate/100) - (growth_rate/100))
    current_price = ticker.history(period="1d")["Close"].iloc[-1]
    margin = (intrinsic_value - current_price) / current_price * 100

    df = ticker.history(period="6mo")
    df["RSI"] = ta.momentum.rsi(df["Close"], window=14)
    rsi = df["RSI"].iloc[-1]
    rsi_signal = "è¶…è³£(å¯èƒ½åå½ˆ)" if rsi < 30 else ("è¶…è²·(å¯èƒ½æ‹‰å›)" if rsi > 70 else "ä¸­æ€§")

    df["MACD"] = ta.trend.macd(df["Close"])
    df["MACD_signal"] = ta.trend.macd_signal(df["Close"])
    macd = df["MACD"].iloc[-1]
    macd_signal = df["MACD_signal"].iloc[-1]
    macd_signal_text = "é»ƒé‡‘äº¤å‰(åå¤š)" if macd > macd_signal else "æ­»äº¡äº¤å‰(åç©º)"



    sentiments = []
    for news in news_list:
        result = sentiment_pipeline(news)[0]
        sentiments.append(f"{news}\næƒ…ç·’: {result['label']} (ä¿¡å¿ƒ: {result['score']:.2f})")

    # ------------------------
    # GPT åˆ†æ
    # GPT åˆ†æ
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    prompt_content = f"""
    ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå¾ä»¥ä¸‹äº”é¢å‘ï¼šè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ï¼Œ
    **ä½ çš„æ–°èå¿…é ˆæ˜¯è¿‘30æ—¥çš„**ï¼Œ
    çµåˆè‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ã€å·´è²ç‰¹è­·åŸæ²³ã€å½¼å¾—æ—å€é¸è‚¡æ–¹æ³•ï¼Œä»¥åŠç¸½é«”ç¶“æ¿Ÿåˆ©ç‡æŠ˜ç¾ç†è«–ï¼Œ
    é‡å° {stock_name} çš„è©³ç´°æŠ•è³‡æ•¸æ“šèˆ‡æŠ€è¡“æŒ‡æ¨™é€²è¡Œåˆ†æï¼Œä¸¦åš´æ ¼ä¾æ“šæ•¸æ“šåšå°ˆæ¥­æ¨è«–ã€‚
     **é‚è¼¯æ¨è«–** èˆ‡ **å…·é«”é æ¸¬**

    ä»¥ä¸‹æ˜¯è¨ˆç®—çµæœï¼š
    - EPS: {eps:.2f}ï¼Œæœ¬ç›Šæ¯”(PE): {pe_ratio:.2f}ï¼Œç”¢æ¥­å¹³å‡PE: {industry_pe:.2f}
    - ä½¿ç”¨ DCF æ¨¡å‹ (FCF={fcf}å„„, g={growth_rate}%, r={discount_rate}%) ä¼°å€¼ç´„ç‚º {intrinsic_value:.2f}
    - ç¾åƒ¹: {current_price:.2f}ï¼Œä¼°å€¼å·®è·ç´„ {margin:.1f}%
    - ROE: éå»5å¹´å¹³å‡ {roe:.2f}% â†’ è­·åŸæ²³: {moat}
    - RSI: {rsi:.2f} â†’ {rsi_signal}ï¼›MACD: {macd:.2f}/{macd_signal:.2f} â†’ {macd_signal_text}
    - ç•¶å‰åˆ©ç‡: {interest_rate:.2f}% ï¼Œè™•æ–¼ {rate_position}ã€‚
    - æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: æ­£å‘/è² å‘/ä¸­æ€§


    ğŸ“°ã€æ–°èæƒ…ç·’ã€‘
    - æœ¬æ—¥æ–°èæƒ…ç·’å¹³å‡ï¼š{avg_sentiment_summary}
    - é‡é»æ–°èæ‘˜è¦å¦‚ä¸‹ï¼š
    """

    for match in matches:
        prompt_content += match["metadata"]["text"] + "\n"

    prompt_content += """
    ä½ éœ€è¦å°‡ä¸Šè¿°æ•¸æ“šèˆ‡æ–°èå®Œæ•´èåˆï¼Œå¾ã€Œè²¡å ±ç©©å¥åº¦ã€ç”¢æ¥­é€±æœŸã€ä¼°å€¼æ°´ä½ã€æŠ€è¡“æŒ‡æ¨™ã€æ–°èæƒ…ç·’ã€
    äº”å¤§é¢å‘é€²è¡Œå°ˆæ¥­åˆ¤è®€ï¼Œä¸¦çµåˆï¼š
    - è‘›æ‹‰æ¼¢å®‰å…¨é‚Šéš›ç†è«–ï¼ˆä½ä¼°é«˜å®‰å…¨é‚Šéš›ï¼‰
    - å·´è²ç‰¹è­·åŸæ²³ï¼ˆROEç©©å¥ã€ç¨ä½”å„ªå‹¢ï¼‰
    - å½¼å¾—æ—å€é¸è‚¡ï¼ˆç”Ÿæ´»åŒ–æ´å¯Ÿèˆ‡æˆé•·æ€§ï¼‰
    - åˆ©ç‡æŠ˜ç¾èˆ‡é€šè†¨é æœŸï¼ˆå®è§€ç¶“æ¿Ÿè§’åº¦ï¼‰


    å¦å¤–è£œå……ä¸€äº›å¸‚å ´å¸¸è­˜ï¼Œä½ å¯ä»¥åœ¨æ¨è«–æ™‚ä¸€ä½µè€ƒæ…®ï¼š
    - è‹¥è¯æº–æœƒ(Fed)é™æ¯ï¼Œé€šå¸¸è¢«è¦–ç‚ºå°è‚¡ç¥¨å¸‚å ´åå¤šçš„åˆ©å¤šã€‚
    - è‹¥CPIæŒçºŒä¸Šæ¼²ï¼Œé¡¯ç¤ºé€šè†¨å£“åŠ›å¢åŠ ï¼Œé€šå¸¸å°è‚¡å¸‚åç©ºã€‚
    - è‹¥ä¼æ¥­æŒçºŒæŠ•å…¥AIèˆ‡è‡ªå‹•åŒ–ï¼Œä»£è¡¨æœªä¾†æˆé•·æ½›åŠ›ï¼Œååˆ©å¤šã€‚
    - æŠ€è¡“é¢è‹¥RSIéä½ã€MACDé»ƒé‡‘äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½åå½ˆï¼›è‹¥RSIéé«˜æˆ–MACDæ­»äº¡äº¤å‰ï¼ŒçŸ­æœŸå¯èƒ½å›æª”ã€‚
    - æœ€æ–°æ–°èæƒ…ç·’å¹³å‡åå‘: {avg_sentiment_summary}

    ä½ éœ€è¦æ˜ç¢ºçš„æŒ‡å‡º
    1.æ˜ç¢ºæ–°èä¾†æº
    åœ¨æç¤ºä¸­æ¨™æ˜æ–°èå¹³å°ï¼æ¨™é¡Œï¼æ™‚é–“ï¼Œè®“æ¨¡å‹çŸ¥é“ä¾æ“šçš„æ˜¯å“ªäº›è³‡è¨Šã€‚

    2.èšç„¦é‡é»å…ƒç´ 
    å¼•å°æ¨¡å‹å…ˆã€Œæ‘˜è¦æ–°èè¦é»ã€ï¼ˆäº‹ä»¶ã€æ•¸æ“šã€æ¶ˆæ¯ï¼‰ï¼Œå†åœ¨æ­¤åŸºç¤ä¸Šæ¨æ¸¬å¿ƒç†ã€‚

    3.æ¡†æ¶åŒ–åˆ†æç¶­åº¦
    å¯ä»¥å¾ã€Œæƒ…ç·’ï¼ˆFear/Greedï¼‰ã€ã€Œé æœŸï¼ˆé æœŸä¸Šæ¼²/ä¸‹è·Œï¼‰ã€ã€Œå‹•æ©Ÿï¼ˆé€¢ä½å¸ƒå±€/ç²åˆ©äº†çµï¼‰ã€ç­‰é¢å‘æ‹†è§£ã€‚

    4.ç”¢å‡ºæ ¼å¼å¼•å°
    æ˜ç¢ºè¦æ±‚ã€Œç”¨æ¢åˆ—ã€æˆ–ã€Œåˆ†æ®µã€å›è¦†ï¼Œæœ‰åŠ©æ–¼çµæ§‹åŒ–çµæœã€‚

    æ­¤å¤–ï¼Œ**è«‹å‹™å¿…é æ¸¬æ˜æ—¥çš„è‚¡åƒ¹æ–¹å‘èˆ‡é ä¼°å¹…åº¦**ï¼ˆä¾‹å¦‚ï¼šã€Œé ä¼°æ˜æ—¥å°å¹…ä¸Šæ¼²ç´„1%ã€æˆ–ã€Œé æœŸä¸‹è·Œ1~2%ã€ï¼‰ï¼Œ
    æœ€å¾Œä»¥1-2å¥è©±çµ¦å‡ºç°¡çŸ­çš„æŠ•è³‡å»ºè­°ï¼ˆå¦‚ã€Œå¯é€¢ä½åˆ†æ‰¹ä½ˆå±€ã€æˆ–ã€Œå»ºè­°çŸ­æœŸè§€æœ›ã€ï¼‰ï¼Œ
    ç¦æ­¢åªæ³›æ³›è€Œè«‡ï¼Œå¿…é ˆå…·é«”è¼¸å‡ºæ¨è«–èˆ‡é æ¸¬æ•¸æ“šã€‚

    è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": prompt_content}
        ]
    )

    print("\n=== GPT å›å‚³åˆ†æ ===")
    print(response.choices[0].message.content)




    # === Flask API ===

    return jsonify({
        "chart": {
            "dates": df.index.strftime("%m/%d").tolist()[-10:],
            "prices": df["Close"].round(2).tolist()[-10:]
        },
        "gpt": response.choices[0].message.content,
        "news": [
            {
                "title": match["metadata"]["text"][:30] + "...",
                "sentiment": match["metadata"]["sentiment"],
                "score": round(match["metadata"]["sentiment_score"], 2)
            }
            for match in matches
        ]
    })



if __name__ == "__main__":
    app.run(port=5000)
